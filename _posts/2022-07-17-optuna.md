---
layout: post

title: "Optuna: hyperparameter optimization"

author: antemrdm

date: 2022-07-17
---

# Introduction

ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬í˜„í•¨ì— ìˆì–´ì„œ hyperparameterë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ë¬¸ì œì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ hyperparameterë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ì„œëŠ” hyperparameterì— ëŒ€í•œ ì—¬ëŸ¬ ë²ˆì˜ ì‹¤í—˜ì„ ì§„í–‰í•©ë‹ˆë‹¤. ì‹¤í—˜ì„ ì§„í–‰í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ì‹¤í—˜ì„ í•  ë•Œë§ˆë‹¤ ì½”ë“œì˜ hyperparameterë“¤ì„ ì§ì ‘ ë³€ê²½í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì„ ì‚¬ìš©í•  ê²½ìš° ìƒˆë¡œìš´ ì‹¤í—˜ì„ ì§„í–‰í•  ë•Œë§ˆë‹¤ ì½”ë“œê°€ ë³€ê²½ë˜ê¸° ë•Œë¬¸ì— ë²„ì „ ê´€ë¦¬ë„ ì‰½ì§€ ì•Šê³ , ë§¤ë²ˆ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ëŠ” ê²ƒì´ ë²ˆê±°ë¡­ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ë³´ë‹¤ ì•½ê°„ ê°œì„ ëœ ë°©ë²•ì€ command line argumentë¡œ hyperparameterë¥¼ ì„¤ì •í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì„ ì‚¬ìš©í•  ê²½ìš°ì—ëŠ” ìƒˆë¡œìš´ ì‹¤í—˜ì„ ì§„í–‰í•  ë•Œë§ˆë‹¤ ì½”ë“œê°€ ë³€ê²½ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” í° ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ê° ì‹¤í—˜ì„ êµ¬ë¶„í•  ë•Œë„ í•´ë‹¹ ì‹¤í—˜ì— ëŒ€í•œ command lineë§Œ ê´€ë¦¬í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— hyperparameterë¥¼ ì§ì ‘ ë³€ê²½í•˜ëŠ” ë°©ë²•ë³´ë‹¤ëŠ” ì‹¤í—˜ì„ ë” ì‰½ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ command line arqumentë¥¼ ì´ìš©í•˜ëŠ” ê²ƒ ì—­ì‹œ ì¶©ë¶„íˆ íš¨ìœ¨ì ì¸ ë°©ë²•ì´ ì•„ë‹™ë‹ˆë‹¤. ì´ ê²½ìš°ì—ë„ hyperparameterì˜ ìˆ˜ê°€ ë§ë‹¤ë©´ í•´ì•¼ í•  ì‹¤í—˜ì˜ ì–‘ì´ ë§ê³ , ê·¸ë¥¼ ì§ì ‘ ìˆ˜í–‰í•˜ê¸°ì—ëŠ” ì–´ë µìŠµë‹ˆë‹¤. ë˜í•œ ì–´ë–¤ ê°’ìœ¼ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•´ì•¼í•˜ëŠ”ì§€, ìµœì ì˜ ì„±ëŠ¥ìœ¼ë¡œ ìˆ˜ë ´í•˜ê³  ìˆëŠ”ì§€, ê° hyperparameter ê°„ì˜ ì—°ê´€ì„±ì´ ìˆëŠì§€, ê° hyperparameterê°€ ì„±ëŠ¥ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë“±ì˜ ë¬¸ì œì— ëŒ€í•œ ë‹µì„ ì°¾ëŠ” ê²ƒì€ ì—¬ì „íˆ ì–´ë ¤ìš´ ë¬¸ì œì´ë©°, ì´ë¡œ ì¸í•´ ìµœì ì˜ hyperparameterë¥¼ ì°¾ëŠ”ë° ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦´ ê²ƒì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë§ì€ ë„êµ¬ë“¤ì´ ê°œë°œë˜ì–´ì™”ê³  í˜„ì¬ì—ë„ ìœ ë§í•œ ë¶„ì•¼ë¡œ ì—°êµ¬ê°€ ì§€ì†ë˜ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ëŠ” hyperparameter íƒìƒ‰ì„ ìœ„í•´ [Hyperopt](https://github.com/hyperopt/hyperopt), [Tune](https://github.com/ray-project/ray), [Hypersearch](https://github.com/kevinzakka/hypersearch), [Skorch](https://github.com/skorch-dev/skorch), [BoTorch](https://botorch.org/), [HiPlot](https://github.com/facebookresearch/hiplot),  [Optuna](https://optuna.org/) ë“±ì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë²ˆ ê¸€ì—ì„œëŠ” ê°€ì¥ ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” **Optuna**ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ì— ëŒ€í•´ ì•Œì•„ë³´ê³ ì í•©ë‹ˆë‹¤.

# Optuna

OptunaëŠ” ì¼ë³¸ì˜ Prefered Networks ì‚¬ì—ì„œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. [ë…¼ë¬¸](https://arxiv.org/abs/1907.10902)ì„ ë³´ë©´ Optunaì— ëŒ€í•´ ìì„¸íˆ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. OptunaëŠ” hyperparameterë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” frameworkë¡œ í™ˆí˜ì´ì§€ì—ëŠ” ì•„ë˜ì™€ ê°™ì´ ì†Œê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

<aside>
ğŸ’¡ An open source hyperparameter optimization framework to automate hyperparameter search

</aside>

Optunaë¥¼ ì•„ì£¼ ê°„ëµí•˜ê²Œ ì„¤ëª…í•˜ë©´ ë‹¨ìˆœí•˜ê²Œ ì—¬ëŸ¬ hyperparameterì— ëŒ€í•´ì„œ ìë™ìœ¼ë¡œ ì‹¤í—˜ì„ ìˆ˜í–‰í•´ì£¼ëŠ” ë„êµ¬ì¸ë° ê° ì‹¤í—˜ì„ trialì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. OptunaëŠ” samplerë¡œ ê° hyperparameterì˜ ê°’ì„ ì„ íƒí•˜ê³ , í•´ë‹¹ ì¡°ê±´ì—ì„œ ì‹¤í—˜(trial)ì„ ìˆ˜í–‰í•œ ë‹¤ìŒ, í•´ë‹¹ ë°©í–¥ìœ¼ë¡œì˜ ì¡°ì •ì„ ê³„ì†í•˜ëŠ” ê²ƒì´ ì¢‹ì€ì§€ë¥¼ prunorë¡œ íŒë‹¨í•˜ì—¬ ìµœì ì— ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨ë˜ë©´ í•´ë‹¹ trialì„ pruneí•˜ê³  ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ë‹¤ì‹œ trialì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.

![https://optuna.readthedocs.io/en/stable/tutorial/index.html](/assets/images/antemrdm/optuna/Untitled.png)

Optunaê°€ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì´ìœ ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

- PyTorch, TensorFlow, Keras ë“± ì—¬ëŸ¬ machine learning frameworkì™€ í•¨ê»˜ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- search spaceì™€ objectiveë¥¼ í•˜ë‚˜ì˜ í•¨ìˆ˜ì— ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì´í›„ì— ì•Œì•„ë³¼ ê²ƒì´ì§€ë§Œ, trialì´ë¼ëŠ” objectë¥¼ ì´ìš©í•´ì„œ ê° hyperparameterì™€ ê·¸ search spaceë¥¼ ì‰½ê²Œ sampleí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë‹¤ì–‘í•œ optimization ë°©ë²•ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.
    - ë‹¤ë¥¸ optimization ë„êµ¬ë“¤ì„ optunaì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - [https://optuna.readthedocs.io/en/stable/reference/integration.html](https://optuna.readthedocs.io/en/stable/reference/integration.html)
- ë‹¤ë¥¸ ë„êµ¬ë“¤ë³´ë‹¤ ì‹œê°í™”ê°€ ì˜ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    - [https://optuna.readthedocs.io/en/stable/reference/visualization/index.html](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html)
- open sourceì´ë©° ë¬´ì—‡ë³´ë‹¤ docsê°€ ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

# Samplers: where to look

ì•„ë¬´ë˜ë„ hyperparameter optimizationì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì€ ì–´ë–¤ ê°’ìœ¼ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•´ì„œ ìµœì ì˜ hyperparameterë¥¼ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì°¾ëŠ”ê°€ì— ê´€í•œ ê²ƒì¼ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ hyperparameterì˜ ê°’ì„ sampleí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì€ hyperparameter optimizationì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

Optunaì—ì„œ samplerê°€ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ì— ê´€í•´ì„œëŠ” [docs](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.BaseSampler.html#optuna.samplers.BaseSampler)ì— ìƒì„¸íˆ ì„¤ëª…ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ìì„¸í•œ ì„¤ëª…ì€ ìƒëµí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. Optunaì—ì„œ ì œê³µí•˜ëŠ” samplerëŠ” [docs](https://optuna.readthedocs.io/en/stable/reference/samplers.html)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆê³ , ë”ë¶ˆì–´ [integration](https://optuna.readthedocs.io/en/stable/reference/integration.html#module-optuna.integration)ì„ ì´ìš©í•´ì„œ ë‹¤ë¥¸ frameworkì˜ sampler ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

samplerëŠ” í¬ê²Œ model-basedì™€ ì•„ë‹Œ ê²ƒìœ¼ë¡œ êµ¬ë¶„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. model-based ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” ì£¼ë¡œ GP(Gaussian Processes), TPE(Tree-structured Parzen Estimator), CMA-ES(Covariance Matrix Adaptation Evolution Strategy) ë“±ì´ ìˆê³ , ê·¸ ì™¸ì—ëŠ” Random Search, Grid Search ë“±ì´ ìˆìŠµë‹ˆë‹¤.

Optunaì—ì„œëŠ” ì•„ë˜ ì¡°ê±´ì— ë”°ë¼ sampler ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ê³  ì œì•ˆí•˜ê³  ìˆìŠµë‹ˆë‹¤.

![https://optuna.readthedocs.io/en/stable/tutorial/index.html](/assets/images/antemrdm/optuna/Untitled1.png)

# Prunors: stopping trials early

samplerì™€ í•¨ê»˜ hyperparameter optimizationì˜ ì¤‘ìš”í•œ ìš”ì†Œ ì¤‘ í•˜ë‚˜ì¸ prunorëŠ” êµ­ë¬¸ìœ¼ë¡œëŠ” ê°€ì§€ë¥¼ ì¹˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦‰ prunor ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ì„œ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ìµœì ì˜ ìƒíƒœì— ë„ë‹¬í•˜ëŠ”ì§€ê°€ ê²°ì •ë©ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ prunorê°€ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![https://optuna.readthedocs.io/en/stable/tutorial/index.html](/assets/images/antemrdm/optuna/Untitled2.png)


ì˜ˆì‹œë¡œ pruning ì„±ëŠ¥ì— ë”°ë¼ì„œ ìµœì ì˜ ìƒíƒœì— ë„ë‹¬í•˜ëŠ” ì‹œê°„ì´ í™•ì—°í•˜ê²Œ ì°¨ì´ê°€ ë‚œë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![https://optuna.readthedocs.io/en/stable/tutorial/index.html](/assets/images/antemrdm/optuna/Untitled3.png)


# êµ¬í˜„

ê·¸ëŸ¼ ì§ì ‘ Optunaë¥¼ ì‚¬ìš©í•´ì„œ hyperparameter optimizationì„ ìˆ˜í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤. ë¬¸ì œ ìƒí™©ì€ ì‚¬ëŒì˜ ì–¼êµ´ ì‚¬ì§„ì„ ë³´ê³  ê·¸ ì‚¬ëŒì˜ ë‚˜ì´ëŒ€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. datasetì€ kaggleì— ê³µê°œëœ datasetì„ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤. frameworkëŠ” pytorchë¥¼ ì‚¬ìš©í•˜ì˜€ê³  ì‚¬ìš©í•œ codeëŠ” [github](https://github.com/junhyeog/optuna-age-prediction)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### dataset

Kaggleì˜ [AGE, GENDER AND ETHNICITY (FACE DATA) CSV](https://www.kaggle.com/datasets/nipunarora8/age-gender-and-ethnicity-face-data-csv)ì´ë¼ëŠ” datasetì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. í•´ë‹¹ datasetì—ëŠ” 23705ê°œì˜ ë°ì´í„°ê°€ ì¡´ì¬í•˜ê³  ê° ë°ì´í„°ëŠ” ì´ë¯¸ì§€, ë‚˜ì´, ì¸ì¢…, ì„±ë³„ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ë‚˜ì´ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í•˜ê¸°ëŠ” ì–´ë µê¸° ë•Œë¬¸ì— 5ê°œì˜ ë²”ìœ„ë¡œ ë‚˜ëˆ„ì—ˆìŠµë‹ˆë‹¤. train dataì™€ test dataë¥¼ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì—ˆìŠµë‹ˆë‹¤. train data, test dataì— ì¡´ì¬í•˜ëŠ” ë‚˜ì´ëŒ€ì˜ ë¹„ìœ¨ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![](/assets/images/antemrdm/optuna/Untitled4.png)

### model

```python
class Model(nn.Module):
    def __init__(self, trial, age_features, ethnicity_features, gender_features):
        super(Model, self).__init__()
        self.name = "Model"

        num_fc_layers = trial.suggest_int("num_fc_layers", 4, 8)
        last_fc = trial.suggest_int("last_fc", 8, 64)

        # fc layer
        self.fc_layers = [nn.Flatten()]

        input_feat = img_size * img_size
        for i in range(num_fc_layers):
            output_feat = trial.suggest_int(f"fc_output_feat_{i}", 8, 64)
            p = trial.suggest_float(f"fc_dropout_{i}", 0, 0.5)

            self.fc_layers.append(nn.Linear(input_feat, output_feat))
            self.fc_layers.append(nn.ReLU())
            self.fc_layers.append(nn.Dropout(p))
            input_feat = output_feat

        self.fc_layers.append(nn.Linear(input_feat, last_fc))

        self.fc_model = nn.Sequential(*self.fc_layers)

        # classifier
        self.age_classifier = nn.Linear(last_fc, age_features)
        self.eth_classifier = nn.Linear(last_fc, ethnicity_features)
        self.gen_classifier = nn.Linear(last_fc, gender_features)

    def forward(self, x):
        output = self.fc_model(x)
        age = self.age_classifier(output)
        eth = self.eth_classifier(output)
        gen = self.gen_classifier(output)
        return age, eth, gen
```

modelì€ ë‹¨ìˆœí•˜ê²Œ FCë§Œì„ ì´ìš©í•´ì„œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì‚¬ìš©ë˜ëŠ” hyperparameterì—ëŠ” num_fc_layers, last_fc, fc_output_feat, fc_dropoutì´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € num_fc_layersëŠ” modelì„ êµ¬ì„±í•  fc layerì˜ ìˆ˜ë¥¼ ì˜ë¯¸í•˜ë©° [4, 8]ì˜ ë²”ìœ„ë¥¼ ê°€ì§€ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤. ê° fc layerëŠ” linear, relu, dropoutìœ¼ë¡œ êµ¬ì„±ì´ ë˜ëŠ”ë°, ì´ë•Œ ì‚¬ìš©ë˜ëŠ” hyperparameterê°€ fc_output_feat, fc_dropoutì…ë‹ˆë‹¤. ê° layerì— ëŒ€í•œ ê°’ì´ êµ¬ë¶„ë  ìˆ˜ ìˆë„ë¡ fc_output_feat_{i}ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê° layerì˜ ë²ˆí˜¸ë¥¼ ê° ë³€ìˆ˜ì˜ ì´ë¦„ ë’¤ì— ë¶™í˜”ìŠµë‹ˆë‹¤. outputì˜ í¬ê¸°ë¥¼ [8, 64] ë²”ìœ„ì—ì„œ ì„ íƒë˜ë„ë¡ í•˜ì˜€ê³ , dropout ë¹„ìœ¨ì€ [0, 0.5] ë²”ìœ„ì˜ ê°’ì„ ê°€ì§€ë„ë¡ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

ë‚˜ì´, ì¸ì¢…, ì„±ë³„ì„ ëª¨ë‘ ì¶”ë¡ í•˜ê¸° ìœ„í•´ì„œ ê° labelì— ëŒ€í•œ linear layerë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì´ë•Œ linear layerì˜ input í¬ê¸°ë¥¼ last_fcë¼ëŠ” hyperparameterë¡œ ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤.

### objective function

```python
def objective(trial):
    model = Model(trial, age_features, eth_features, gen_features).to(device)
    opt_name = trial.suggest_categorical(
        "optimizer",
        ["Adam", "Adadelta", "RMSprop", "SGD", "MADGRAD"],
    )
    lr = trial.suggest_float("lr", 1e-4, 1e-2, log=True)
    if opt_name == "MADGRAD":
        optimizer = madgrad.MADGRAD(model.parameters(), lr=lr)
    else:
        optimizer = getattr(optim, opt_name)(model.parameters(), lr=lr)

    for epoch in range(1, n_epochs + 1):
        train(model, train_dataloader, optimizer, epoch, weight_path=None, quiet=(epoch % period))
        accuracy = test(model, test_dataloader, quiet=True)
        trial.report(accuracy, epoch)
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()

    return accuracy
```

objective í•¨ìˆ˜ì—ì„œ ì¶”ê°€ì ìœ¼ë¡œ ì •ì˜í•œ hyperparameterëŠ” lrê³¼ opt_nameì…ë‹ˆë‹¤. lrì€ learning rateë¡œ, [1e-4, 1e-2] ë²”ìœ„ë¦ ê°’ì„ ê°€ì§€ë„ë¡ í•˜ì˜€ê³ , opt_nameì€ suggest_categorical í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ Adam, Adadelta, RMSprop, SGD, MADGRAD ì¤‘ì—ì„œ optimizerê°€ ì„ íƒë˜ë„ë¡ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.

ê° trialì—ì„œ ë¯¸ë¦¬ ì •ì˜í•œ n_epochsë§Œí¼ì˜ í•™ìŠµì„ ì§„í–‰í•˜ê²Œ ë˜ê³ , ê·¸ í›„ testí•œ ê²°ê³¼ì— ë”°ë¼ì„œ í•´ë‹¹ trialì´ pruneë ì§€ ì•ˆë ì§€ê°€ ê²°ì •ë©ë‹ˆë‹¤.

### study

```python
storage = "sqlite:///test.db"
study_name = "001"

sampler = SkoptSampler(
    skopt_kwargs={
        "base_estimator": "RF",
        "n_random_starts": 10,
        "base_estimator": "ET",
        "acq_func": "EI",
        "acq_func_kwargs": {"xi": 0.02},
    },
    warn_independent_sampling=False
)

study = optuna.create_study(study_name=study_name, direction="maximize", storage=storage, sampler=sampler, load_if_exists=True)

study.optimize(objective, n_trials=20)
```

Optunaì—ì„œëŠ” study ê°ì²´ì—ì„œ hyperparameter optimizationì´ ì§„í–‰ë©ë‹ˆë‹¤. samplerë¡œëŠ” scikit optimizeì˜ SkoptSamplerë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

skoptëŠ” Bayesian optimizationì´ë©°, ì´ë•Œ acq_funcëŠ” Acquisition Functionì„ ì˜ë¯¸í•˜ê³  ë‹¤ìŒ ê°’ì„ ê²°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” Expected Improvement(EI)ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

### beat trial

```python
pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]
complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]

print("num_trials_conducted: ", len(study.trials))
print("num_trials_pruned: ", len(pruned_trials))
print("num_trials_completed: ", len(complete_trials))

trial = study.best_trial
print("[+] results: -----------------------------------------")
print(" | results from best trial:")

print(" | accuracy: ", trial.value)
print(" | hyperparameters: ")
for key, value in trial.params.items():
    print(f" | {key}: {value}")
print(" +----------------------------------------------------")
```

```
num_trials_conducted:  20
num_trials_pruned:  15
num_trials_completed:  5
[+] results: -----------------------------------------
 | results from best trial:
 | accuracy:  69.45792026998524
 | hyperparameters: 
 | fc_dropout_0: 0.05927094105851344
 | fc_dropout_1: 0.29084040718075477
 | fc_dropout_2: 0.13455450122716334
 | fc_dropout_3: 0.29114171437625075
 | fc_output_feat_0: 36
 | fc_output_feat_1: 40
 | fc_output_feat_2: 28
 | fc_output_feat_3: 34
 | last_fc: 43
 | lr: 0.0015423462806943048
 | num_fc_layers: 4
 | optimizer: MADGRAD
 +----------------------------------------------------
```

ìœ„ ê²°ê³¼ë¥¼ ë¶„ì„í•´ë³´ë©´ 20ë²ˆì˜ trialì„ ì§„í–‰í•˜ì˜€ê³ , ê·¸ ì¤‘ì—ì„œ 15ê°œê°€ pruneë˜ì—ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ì¥ accuracyê°€ ë†’ì•˜ë˜ trialì—ì„œ ì‚¬ìš©ëœ hyperparameterì˜ ê°’ ë˜í•œ ì•Œ ìˆ˜ ìˆê³ , ì´ë¥¼ ìµœì ì˜ hyperparameterë¡œ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# Skopt Sampler vs Ramdom Sampler + Visualization

samplerê°€ optimization ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ Skopt Samplerì™€ Ramdom Samplerì˜ ì„±ëŠ¥ì— ëŒ€í•´ ë¹„êµë¥¼ í•´ë³´ì•˜ìŠµë‹ˆë‹¤.

ë˜í•œ Optunaì—ì„œ ì œê³µí•˜ëŠ” ì‹œê°í™” ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ì‹œê°í™”í•´ë³´ì•˜ìŠµë‹ˆë‹¤.

### best trial

- Skopt Sampler
    
    ```
    num_trials_conducted:  100
    num_trials_pruned:  73
    num_trials_completed:  27
    [+] results: -----------------------------------------
     | results from best trial:
     | accuracy:  69.56338325247837
     | hyperparameters: 
     | fc_dropout_0: 0.13376861811536073
     | fc_dropout_1: 0.15357136269549693
     | fc_dropout_2: 0.07170506034936153
     | fc_dropout_3: 0.04104589071975262
     | fc_dropout_4: 0.38519700327029344
     | fc_output_feat_0: 55
     | fc_output_feat_1: 53
     | fc_output_feat_2: 62
     | fc_output_feat_3: 62
     | fc_output_feat_4: 28
     | last_fc: 55
     | lr: 0.002676347859117992
     | num_fc_layers: 5
     | optimizer: Adam
     +----------------------------------------------------
    ```
    
- Ramdom Sampler
    
    ```
    num_trials_conducted:  100
    num_trials_pruned:  87
    num_trials_completed:  13
    [+] results: -----------------------------------------
    | results from best trial:
    | accuracy:  67.60177177810588
    | hyperparameters:
    | fc_dropout_0: 0.016225409134656366
    | fc_dropout_1: 0.4284657526506286
    | fc_dropout_2: 0.13551661039530455
    | fc_dropout_3: 0.1982516747418307
    | fc_dropout_4: 0.04493982076202707
    | fc_dropout_5: 0.062072207295049364
    | fc_output_feat_0: 36
    | fc_output_feat_1: 61
    | fc_output_feat_2: 49
    | fc_output_feat_3: 46
    | fc_output_feat_4: 57
    | fc_output_feat_5: 13
    | last_fc: 24
    | lr: 0.0001700730078227611
    | num_fc_layers: 6
    | optimizer: MADGRAD
    +----------------------------------------------------
    ```
    

Skopt Samplerë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ ë” ë†’ì€ accuracyë¥¼ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

### optimization history

- Skopt Sampler
    
    ![](/assets/images/antemrdm/optuna/skopt_plot_optimization_history.png)
    
- Ramdom Sampler
    
    ![](/assets/images/antemrdm/optuna/random_plot_optimization_history.png)
    

Skopt Samplerë¥¼ ì‚¬ìš©í–ˆì„ ë•ŒëŠ” best trialì´ ë§ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆì§€ë§Œ, Ramdom Samplerë¥¼ ì‚¬ìš©í–ˆì„ ë•ŒëŠ” ê·¸ëŸ¬ì§€ ì•ŠìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### hyperparameter importances

- Skopt Sampler
    
    ![](/assets/images/antemrdm/optuna/skopt_plot_param_importances.png)
    
- Ramdom Sampler
    
    ![](/assets/images/antemrdm/optuna/random_plot_param_importances.png)
    

ìœ„ ê·¸ë˜í”„ëŠ” [optuna.visualization.plot_param_importances](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_param_importances.html#optuna.visualization.plot_param_importances) í•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ ê° hyperparameterì˜ ì¤‘ìš”ë„ë¥¼ í™•ì¸í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ë‘ ê·¸ë˜í”„ ëª¨ë‘ optimizerê°€ ì••ë„ì ìœ¼ë¡œ ì¤‘ìš”í•˜ë‹¤ê³  ë§í•˜ê³  ìˆê³ , ê·¸ì— ë¹„í•´ ë‚˜ë¨¸ì§€ hyperparameterì˜ ì¤‘ìš”ë„ëŠ” ë‘ ê·¸ë˜í”„ì—ì„œ ìœ ì‚¬í•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# Reference

- [https://optuna.org/](https://optuna.org/)
- Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta,and Masanori Koyama. 2019.
Optuna: A Next-generation Hyperparameter Optimization Framework. In KDD.
- [https://neptune.ai/blog/optuna-vs-hyperopt](https://neptune.ai/blog/optuna-vs-hyperopt)
- [https://optuna.readthedocs.io/en/stable/index.html](https://optuna.readthedocs.io/en/stable/index.html)
